{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from pandas import DataFrame, read_csv\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import wfdb\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.image as mpimg\n",
    "import tkinter as tk\n",
    "from matplotlib.figure import Figure\n",
    "from IPython.display import display, HTML\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.ticker as ticker\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import tensorflow.keras as ks\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D,Conv1D, MaxPooling1D, BatchNormalization\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras import backend as Ks\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,CSVLogger\n",
    "from numpy.random import seed\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv('TrainingSeg.csv',header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1014</th>\n",
       "      <th>1015</th>\n",
       "      <th>1016</th>\n",
       "      <th>1017</th>\n",
       "      <th>1018</th>\n",
       "      <th>1019</th>\n",
       "      <th>1020</th>\n",
       "      <th>1021</th>\n",
       "      <th>1022</th>\n",
       "      <th>1023</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19.0</td>\n",
       "      <td>15.4</td>\n",
       "      <td>16.6</td>\n",
       "      <td>14.5</td>\n",
       "      <td>15.8</td>\n",
       "      <td>15.9</td>\n",
       "      <td>16.5</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>17.8</td>\n",
       "      <td>...</td>\n",
       "      <td>19.3</td>\n",
       "      <td>17.70</td>\n",
       "      <td>18.40</td>\n",
       "      <td>18.60</td>\n",
       "      <td>20.40</td>\n",
       "      <td>19.6</td>\n",
       "      <td>18.9</td>\n",
       "      <td>18.6</td>\n",
       "      <td>17.8</td>\n",
       "      <td>19.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96.0</td>\n",
       "      <td>96.1</td>\n",
       "      <td>96.8</td>\n",
       "      <td>97.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>97.9</td>\n",
       "      <td>98.0</td>\n",
       "      <td>97.8</td>\n",
       "      <td>...</td>\n",
       "      <td>98.6</td>\n",
       "      <td>98.52</td>\n",
       "      <td>98.44</td>\n",
       "      <td>98.36</td>\n",
       "      <td>98.28</td>\n",
       "      <td>98.2</td>\n",
       "      <td>97.8</td>\n",
       "      <td>99.0</td>\n",
       "      <td>98.2</td>\n",
       "      <td>98.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>106.3</td>\n",
       "      <td>103.8</td>\n",
       "      <td>104.4</td>\n",
       "      <td>103.4</td>\n",
       "      <td>103.5</td>\n",
       "      <td>102.7</td>\n",
       "      <td>102.2</td>\n",
       "      <td>101.8</td>\n",
       "      <td>101.1</td>\n",
       "      <td>101.4</td>\n",
       "      <td>...</td>\n",
       "      <td>76.2</td>\n",
       "      <td>75.20</td>\n",
       "      <td>76.10</td>\n",
       "      <td>77.90</td>\n",
       "      <td>71.60</td>\n",
       "      <td>74.5</td>\n",
       "      <td>72.4</td>\n",
       "      <td>70.6</td>\n",
       "      <td>75.0</td>\n",
       "      <td>76.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>10.2</td>\n",
       "      <td>10.6</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.60</td>\n",
       "      <td>11.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.4</td>\n",
       "      <td>17.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1024 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3      4      5      6      7      8      9  ...  \\\n",
       "0   19.0   15.4   16.6   14.5   15.8   15.9   16.5   15.0   15.6   17.8  ...   \n",
       "1   96.0   96.1   96.8   97.0   97.0   97.0   97.0   97.9   98.0   97.8  ...   \n",
       "2  106.3  103.8  104.4  103.4  103.5  102.7  102.2  101.8  101.1  101.4  ...   \n",
       "3    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "4   10.0   10.0   10.0   10.0   10.0   10.0   10.5   10.2   10.6   10.0  ...   \n",
       "\n",
       "   1014   1015   1016   1017   1018  1019  1020  1021  1022  1023  \n",
       "0  19.3  17.70  18.40  18.60  20.40  19.6  18.9  18.6  17.8  19.2  \n",
       "1  98.6  98.52  98.44  98.36  98.28  98.2  97.8  99.0  98.2  98.4  \n",
       "2  76.2  75.20  76.10  77.90  71.60  74.5  72.4  70.6  75.0  76.5  \n",
       "3   0.0   0.00   0.00   0.00   0.00   0.0   0.0   0.0   0.0   0.0  \n",
       "4  10.0  10.60  11.00  11.00  10.00  10.0  10.1  11.0  12.4  17.1  \n",
       "\n",
       "[5 rows x 1024 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Normalization Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parameter</th>\n",
       "      <th>Mean</th>\n",
       "      <th>StdDev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RR</td>\n",
       "      <td>20.073284</td>\n",
       "      <td>6.376141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SpO2</td>\n",
       "      <td>96.074189</td>\n",
       "      <td>4.515870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HR</td>\n",
       "      <td>86.446392</td>\n",
       "      <td>18.010956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Parameter       Mean     StdDev\n",
       "0        RR  20.073284   6.376141\n",
       "1      SpO2  96.074189   4.515870\n",
       "2        HR  86.446392  18.010956"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "20.07328408143529"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "normalize_param=pd.read_csv('parStat.csv',header=0)\n",
    "display(normalize_param)\n",
    "display(normalize_param.iloc[0,1])\n",
    "mean_rr,std_rr=normalize_param.iloc[0,1],normalize_param.iloc[0,2]\n",
    "mean_spo2,std_spo2=normalize_param.iloc[1,1],normalize_param.iloc[1,2]\n",
    "mean_hr,std_hr=normalize_param.iloc[2,1],normalize_param.iloc[2,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to normalize the given dataframe using respective mean,std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(df):\n",
    "    row_index=df.index\n",
    "    for i in range(len(row_index)):\n",
    "        #display(i)\n",
    "        if i%4==0:\n",
    "            #print(df.iloc[i,:])\n",
    "            df.iloc[i,:]=df.iloc[i,:].apply(lambda x:(x-mean_rr)/std_rr)\n",
    "            #print(df.iloc[i,:])\n",
    "        elif i%4==1:\n",
    "            df.iloc[i,:]=df.iloc[i,:].apply(lambda x:(x-mean_spo2)/std_spo2)\n",
    "        elif i%4==2:\n",
    "            df.iloc[i,:]=df.iloc[i,:].apply(lambda x:(x-mean_hr)/std_hr)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1014</th>\n",
       "      <th>1015</th>\n",
       "      <th>1016</th>\n",
       "      <th>1017</th>\n",
       "      <th>1018</th>\n",
       "      <th>1019</th>\n",
       "      <th>1020</th>\n",
       "      <th>1021</th>\n",
       "      <th>1022</th>\n",
       "      <th>1023</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.168328</td>\n",
       "      <td>-0.732933</td>\n",
       "      <td>-0.544731</td>\n",
       "      <td>-0.874084</td>\n",
       "      <td>-0.670199</td>\n",
       "      <td>-0.654516</td>\n",
       "      <td>-0.560415</td>\n",
       "      <td>-0.795667</td>\n",
       "      <td>-0.701566</td>\n",
       "      <td>-0.356530</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.121278</td>\n",
       "      <td>-0.372213</td>\n",
       "      <td>-0.262429</td>\n",
       "      <td>-0.231062</td>\n",
       "      <td>0.051240</td>\n",
       "      <td>-0.074227</td>\n",
       "      <td>-0.184012</td>\n",
       "      <td>-0.231062</td>\n",
       "      <td>-0.356530</td>\n",
       "      <td>-0.136961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.016428</td>\n",
       "      <td>0.005716</td>\n",
       "      <td>0.160725</td>\n",
       "      <td>0.205013</td>\n",
       "      <td>0.205013</td>\n",
       "      <td>0.205013</td>\n",
       "      <td>0.205013</td>\n",
       "      <td>0.404310</td>\n",
       "      <td>0.426454</td>\n",
       "      <td>0.382166</td>\n",
       "      <td>...</td>\n",
       "      <td>0.559319</td>\n",
       "      <td>0.541604</td>\n",
       "      <td>0.523888</td>\n",
       "      <td>0.506173</td>\n",
       "      <td>0.488458</td>\n",
       "      <td>0.470742</td>\n",
       "      <td>0.382166</td>\n",
       "      <td>0.647895</td>\n",
       "      <td>0.470742</td>\n",
       "      <td>0.515031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.102307</td>\n",
       "      <td>0.963503</td>\n",
       "      <td>0.996816</td>\n",
       "      <td>0.941294</td>\n",
       "      <td>0.946846</td>\n",
       "      <td>0.902429</td>\n",
       "      <td>0.874668</td>\n",
       "      <td>0.852459</td>\n",
       "      <td>0.813594</td>\n",
       "      <td>0.830251</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.568898</td>\n",
       "      <td>-0.624419</td>\n",
       "      <td>-0.574450</td>\n",
       "      <td>-0.474511</td>\n",
       "      <td>-0.824298</td>\n",
       "      <td>-0.663285</td>\n",
       "      <td>-0.779880</td>\n",
       "      <td>-0.879820</td>\n",
       "      <td>-0.635524</td>\n",
       "      <td>-0.552241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.579840</td>\n",
       "      <td>-1.579840</td>\n",
       "      <td>-1.579840</td>\n",
       "      <td>-1.579840</td>\n",
       "      <td>-1.579840</td>\n",
       "      <td>-1.579840</td>\n",
       "      <td>-1.501423</td>\n",
       "      <td>-1.548473</td>\n",
       "      <td>-1.485740</td>\n",
       "      <td>-1.579840</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.579840</td>\n",
       "      <td>-1.485740</td>\n",
       "      <td>-1.423006</td>\n",
       "      <td>-1.423006</td>\n",
       "      <td>-1.579840</td>\n",
       "      <td>-1.579840</td>\n",
       "      <td>-1.564157</td>\n",
       "      <td>-1.423006</td>\n",
       "      <td>-1.203437</td>\n",
       "      <td>-0.466314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1024 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -0.168328 -0.732933 -0.544731 -0.874084 -0.670199 -0.654516 -0.560415   \n",
       "1 -0.016428  0.005716  0.160725  0.205013  0.205013  0.205013  0.205013   \n",
       "2  1.102307  0.963503  0.996816  0.941294  0.946846  0.902429  0.874668   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4 -1.579840 -1.579840 -1.579840 -1.579840 -1.579840 -1.579840 -1.501423   \n",
       "\n",
       "          7         8         9  ...      1014      1015      1016      1017  \\\n",
       "0 -0.795667 -0.701566 -0.356530  ... -0.121278 -0.372213 -0.262429 -0.231062   \n",
       "1  0.404310  0.426454  0.382166  ...  0.559319  0.541604  0.523888  0.506173   \n",
       "2  0.852459  0.813594  0.830251  ... -0.568898 -0.624419 -0.574450 -0.474511   \n",
       "3  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "4 -1.548473 -1.485740 -1.579840  ... -1.579840 -1.485740 -1.423006 -1.423006   \n",
       "\n",
       "       1018      1019      1020      1021      1022      1023  \n",
       "0  0.051240 -0.074227 -0.184012 -0.231062 -0.356530 -0.136961  \n",
       "1  0.488458  0.470742  0.382166  0.647895  0.470742  0.515031  \n",
       "2 -0.824298 -0.663285 -0.779880 -0.879820 -0.635524 -0.552241  \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "4 -1.579840 -1.579840 -1.564157 -1.423006 -1.203437 -0.466314  \n",
       "\n",
       "[5 rows x 1024 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# normalizing train data\n",
    "train_data=normalize_data(train_data)\n",
    "display(train_data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1014</th>\n",
       "      <th>1015</th>\n",
       "      <th>1016</th>\n",
       "      <th>1017</th>\n",
       "      <th>1018</th>\n",
       "      <th>1019</th>\n",
       "      <th>1020</th>\n",
       "      <th>1021</th>\n",
       "      <th>1022</th>\n",
       "      <th>1023</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.341003</td>\n",
       "      <td>2.341003</td>\n",
       "      <td>2.341003</td>\n",
       "      <td>0.587115</td>\n",
       "      <td>0.080020</td>\n",
       "      <td>0.678585</td>\n",
       "      <td>1.295477</td>\n",
       "      <td>0.668113</td>\n",
       "      <td>0.362286</td>\n",
       "      <td>0.268197</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.508153</td>\n",
       "      <td>-1.057056</td>\n",
       "      <td>-1.080538</td>\n",
       "      <td>-1.143291</td>\n",
       "      <td>-1.070146</td>\n",
       "      <td>-1.119808</td>\n",
       "      <td>-1.219133</td>\n",
       "      <td>-1.266177</td>\n",
       "      <td>-1.347176</td>\n",
       "      <td>-1.313221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.680721</td>\n",
       "      <td>-0.680721</td>\n",
       "      <td>-0.621674</td>\n",
       "      <td>-0.529434</td>\n",
       "      <td>-0.422367</td>\n",
       "      <td>-0.459305</td>\n",
       "      <td>-0.654918</td>\n",
       "      <td>-0.765626</td>\n",
       "      <td>-1.041604</td>\n",
       "      <td>-0.741405</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.079162</td>\n",
       "      <td>-0.090243</td>\n",
       "      <td>-0.060693</td>\n",
       "      <td>-0.105018</td>\n",
       "      <td>-0.079162</td>\n",
       "      <td>-0.082856</td>\n",
       "      <td>-0.020114</td>\n",
       "      <td>-0.038583</td>\n",
       "      <td>-0.056999</td>\n",
       "      <td>-0.045918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.131830</td>\n",
       "      <td>-1.131830</td>\n",
       "      <td>-1.131830</td>\n",
       "      <td>-1.131830</td>\n",
       "      <td>-1.268488</td>\n",
       "      <td>-1.286048</td>\n",
       "      <td>-1.028819</td>\n",
       "      <td>-0.907611</td>\n",
       "      <td>-0.910318</td>\n",
       "      <td>-1.016794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.531881</td>\n",
       "      <td>-0.544862</td>\n",
       "      <td>-0.705849</td>\n",
       "      <td>-0.741964</td>\n",
       "      <td>-0.753074</td>\n",
       "      <td>-0.754906</td>\n",
       "      <td>-0.779872</td>\n",
       "      <td>-0.799303</td>\n",
       "      <td>-0.791937</td>\n",
       "      <td>-0.836335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.078000</td>\n",
       "      <td>-1.109336</td>\n",
       "      <td>-1.130280</td>\n",
       "      <td>-1.106718</td>\n",
       "      <td>-1.125044</td>\n",
       "      <td>-0.879271</td>\n",
       "      <td>0.349196</td>\n",
       "      <td>0.065105</td>\n",
       "      <td>-0.157899</td>\n",
       "      <td>-0.450637</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.064910</td>\n",
       "      <td>-0.743374</td>\n",
       "      <td>-1.096246</td>\n",
       "      <td>-1.122426</td>\n",
       "      <td>-1.158998</td>\n",
       "      <td>-1.195571</td>\n",
       "      <td>-1.098864</td>\n",
       "      <td>-1.185099</td>\n",
       "      <td>-1.177324</td>\n",
       "      <td>-1.151145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1024 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  2.341003  2.341003  2.341003  0.587115  0.080020  0.678585  1.295477   \n",
       "1 -0.680721 -0.680721 -0.621674 -0.529434 -0.422367 -0.459305 -0.654918   \n",
       "2 -1.131830 -1.131830 -1.131830 -1.131830 -1.268488 -1.286048 -1.028819   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4 -1.078000 -1.109336 -1.130280 -1.106718 -1.125044 -0.879271  0.349196   \n",
       "\n",
       "          7         8         9  ...      1014      1015      1016      1017  \\\n",
       "0  0.668113  0.362286  0.268197  ... -0.508153 -1.057056 -1.080538 -1.143291   \n",
       "1 -0.765626 -1.041604 -0.741405  ... -0.079162 -0.090243 -0.060693 -0.105018   \n",
       "2 -0.907611 -0.910318 -1.016794  ... -0.531881 -0.544862 -0.705849 -0.741964   \n",
       "3  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.065105 -0.157899 -0.450637  ... -1.064910 -0.743374 -1.096246 -1.122426   \n",
       "\n",
       "       1018      1019      1020      1021      1022      1023  \n",
       "0 -1.070146 -1.119808 -1.219133 -1.266177 -1.347176 -1.313221  \n",
       "1 -0.079162 -0.082856 -0.020114 -0.038583 -0.056999 -0.045918  \n",
       "2 -0.753074 -0.754906 -0.779872 -0.799303 -0.791937 -0.836335  \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "4 -1.158998 -1.195571 -1.098864 -1.185099 -1.177324 -1.151145  \n",
       "\n",
       "[5 rows x 1024 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reading and normalizing validation data\n",
    "\n",
    "validation_data=pd.read_csv('ValidationSeg.csv',header=0)\n",
    "validation_data=normalize_data(validation_data)\n",
    "display(validation_data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1014</th>\n",
       "      <th>1015</th>\n",
       "      <th>1016</th>\n",
       "      <th>1017</th>\n",
       "      <th>1018</th>\n",
       "      <th>1019</th>\n",
       "      <th>1020</th>\n",
       "      <th>1021</th>\n",
       "      <th>1022</th>\n",
       "      <th>1023</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.169500</td>\n",
       "      <td>-1.169500</td>\n",
       "      <td>-0.011547</td>\n",
       "      <td>-1.632089</td>\n",
       "      <td>1.578194</td>\n",
       "      <td>0.834262</td>\n",
       "      <td>0.438011</td>\n",
       "      <td>0.223794</td>\n",
       "      <td>-0.748372</td>\n",
       "      <td>-0.615302</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.432280</td>\n",
       "      <td>0.025175</td>\n",
       "      <td>-0.288548</td>\n",
       "      <td>-0.638796</td>\n",
       "      <td>-0.625766</td>\n",
       "      <td>-0.680652</td>\n",
       "      <td>0.362393</td>\n",
       "      <td>-0.487167</td>\n",
       "      <td>-0.644127</td>\n",
       "      <td>-0.461105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.623251</td>\n",
       "      <td>0.607319</td>\n",
       "      <td>0.588796</td>\n",
       "      <td>0.607707</td>\n",
       "      <td>0.626618</td>\n",
       "      <td>0.526105</td>\n",
       "      <td>0.566777</td>\n",
       "      <td>0.537244</td>\n",
       "      <td>0.666383</td>\n",
       "      <td>0.670010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.869223</td>\n",
       "      <td>0.869223</td>\n",
       "      <td>0.869223</td>\n",
       "      <td>0.869223</td>\n",
       "      <td>0.869223</td>\n",
       "      <td>0.869223</td>\n",
       "      <td>0.869223</td>\n",
       "      <td>0.869223</td>\n",
       "      <td>0.869223</td>\n",
       "      <td>0.869223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.087672</td>\n",
       "      <td>-0.962126</td>\n",
       "      <td>-1.037255</td>\n",
       "      <td>-1.024189</td>\n",
       "      <td>-1.019929</td>\n",
       "      <td>-0.562481</td>\n",
       "      <td>-0.493034</td>\n",
       "      <td>-0.398448</td>\n",
       "      <td>-0.759464</td>\n",
       "      <td>-0.743841</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014567</td>\n",
       "      <td>0.030737</td>\n",
       "      <td>-0.042403</td>\n",
       "      <td>-0.229302</td>\n",
       "      <td>-0.226462</td>\n",
       "      <td>-0.304289</td>\n",
       "      <td>0.205564</td>\n",
       "      <td>-0.091400</td>\n",
       "      <td>-0.280145</td>\n",
       "      <td>-0.246912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.785293</td>\n",
       "      <td>-0.795757</td>\n",
       "      <td>-0.795757</td>\n",
       "      <td>-0.795757</td>\n",
       "      <td>-0.795757</td>\n",
       "      <td>-0.795757</td>\n",
       "      <td>-0.795757</td>\n",
       "      <td>-0.795757</td>\n",
       "      <td>-0.795757</td>\n",
       "      <td>-0.795757</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179372</td>\n",
       "      <td>0.391021</td>\n",
       "      <td>0.289145</td>\n",
       "      <td>-0.173641</td>\n",
       "      <td>-0.398321</td>\n",
       "      <td>-0.228528</td>\n",
       "      <td>-0.168310</td>\n",
       "      <td>-0.074134</td>\n",
       "      <td>-0.113423</td>\n",
       "      <td>0.116588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1024 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -1.169500 -1.169500 -0.011547 -1.632089  1.578194  0.834262  0.438011   \n",
       "1  0.623251  0.607319  0.588796  0.607707  0.626618  0.526105  0.566777   \n",
       "2 -1.087672 -0.962126 -1.037255 -1.024189 -1.019929 -0.562481 -0.493034   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4 -0.785293 -0.795757 -0.795757 -0.795757 -0.795757 -0.795757 -0.795757   \n",
       "\n",
       "          7         8         9  ...      1014      1015      1016      1017  \\\n",
       "0  0.223794 -0.748372 -0.615302  ... -0.432280  0.025175 -0.288548 -0.638796   \n",
       "1  0.537244  0.666383  0.670010  ...  0.869223  0.869223  0.869223  0.869223   \n",
       "2 -0.398448 -0.759464 -0.743841  ... -0.014567  0.030737 -0.042403 -0.229302   \n",
       "3  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "4 -0.795757 -0.795757 -0.795757  ...  0.179372  0.391021  0.289145 -0.173641   \n",
       "\n",
       "       1018      1019      1020      1021      1022      1023  \n",
       "0 -0.625766 -0.680652  0.362393 -0.487167 -0.644127 -0.461105  \n",
       "1  0.869223  0.869223  0.869223  0.869223  0.869223  0.869223  \n",
       "2 -0.226462 -0.304289  0.205564 -0.091400 -0.280145 -0.246912  \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "4 -0.398321 -0.228528 -0.168310 -0.074134 -0.113423  0.116588  \n",
       "\n",
       "[5 rows x 1024 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reading and normalizing test data\n",
    "\n",
    "test_data=pd.read_csv('TestSeg.csv',header=0)\n",
    "test_data=normalize_data(test_data)\n",
    "display(test_data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to convert the given data to required 3D input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_3darray(df):\n",
    "    df=df.T\n",
    "    columns=df.columns\n",
    "    segments=[]\n",
    "    for i in range(0,len(columns),4):\n",
    "        col_subset=columns[i:i+4]\n",
    "        segments.append(df[col_subset].values)\n",
    "    display(segments[0]) \n",
    "    #display(type(segments[0]))\n",
    "    #display(type(segments))\n",
    "    return np.asarray(segments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.16832817, -0.01642845,  1.10230725,  0.        ],\n",
       "       [-0.73293303,  0.00571568,  0.96350285,  0.        ],\n",
       "       [-0.54473141,  0.16072456,  0.99681591,  0.        ],\n",
       "       ...,\n",
       "       [-0.23106204,  0.64789532, -0.87981957,  0.        ],\n",
       "       [-0.35652979,  0.47074232, -0.63552383,  0.        ],\n",
       "       [-0.13696123,  0.51503057, -0.55224119,  0.        ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1707, 1024, 4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# converting train data to 3D Array\n",
    "train_data_final=convert_to_3darray(train_data)\n",
    "display(train_data_final.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.34100261, -0.68072132, -1.13182999,  0.        ],\n",
       "       [ 2.34100261, -0.68072132, -1.13182999,  0.        ],\n",
       "       [ 2.34100261, -0.62167352, -1.13182999,  0.        ],\n",
       "       ...,\n",
       "       [-1.26617697, -0.0385831 , -0.79930349,  0.        ],\n",
       "       [-1.34717568, -0.05699926, -0.79193702,  0.        ],\n",
       "       [-1.31322128, -0.0459179 , -0.83633494,  0.        ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(446, 1024, 4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# converting validation data to 3D Array\n",
    "val_data_final=convert_to_3darray(validation_data)\n",
    "display(val_data_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.16949991,  0.62325059, -1.08767235,  0.        ],\n",
       "       [-1.16949991,  0.60731871, -0.96212645,  0.        ],\n",
       "       [-0.01154733,  0.58879629, -1.03725516,  0.        ],\n",
       "       ...,\n",
       "       [-0.48716673,  0.86922321, -0.09140025,  0.        ],\n",
       "       [-0.64412705,  0.86922321, -0.28014517,  0.        ],\n",
       "       [-0.46110539,  0.86922321, -0.24691243,  0.        ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(659, 1024, 4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# converting test data to 3D Array\n",
    "test_data_final=convert_to_3darray(test_data)\n",
    "display(test_data_final.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  spillting data into features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_features_label(arr):\n",
    "    x=arr[:,:,:3]\n",
    "    y=arr[:,:,3:]\n",
    "    display(x.shape)\n",
    "    display(y.shape)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1707, 1024, 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1707, 1024, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(446, 1024, 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(446, 1024, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(659, 1024, 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(659, 1024, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# splitting train,test and validation data into features and labels\n",
    "x_train,y_train=split_features_label(train_data_final)\n",
    "x_val,y_val=split_features_label(val_data_final)\n",
    "x_test,y_test=split_features_label(test_data_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 1024, 100)         41600     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024, 1)           101       \n",
      "=================================================================\n",
      "Total params: 41,701\n",
      "Trainable params: 41,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1707 samples, validate on 446 samples\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.12363, saving model to SegLSTM.h5\n",
      "1707/1707 [==============================] - 3s 2ms/sample - loss: 0.2625 - val_loss: 0.1236\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.12363 to 0.12280, saving model to SegLSTM.h5\n",
      "1707/1707 [==============================] - 0s 288us/sample - loss: 0.2140 - val_loss: 0.1228\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.12280\n",
      "1707/1707 [==============================] - 0s 269us/sample - loss: 0.1808 - val_loss: 0.1261\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.12280\n",
      "1707/1707 [==============================] - 0s 271us/sample - loss: 0.1596 - val_loss: 0.1296\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.12280\n",
      "1707/1707 [==============================] - 0s 263us/sample - loss: 0.1468 - val_loss: 0.1295\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.12280\n",
      "1707/1707 [==============================] - 0s 274us/sample - loss: 0.1380 - val_loss: 0.1247\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.12280 to 0.11676, saving model to SegLSTM.h5\n",
      "1707/1707 [==============================] - 1s 293us/sample - loss: 0.1303 - val_loss: 0.1168\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.11676 to 0.10819, saving model to SegLSTM.h5\n",
      "1707/1707 [==============================] - 0s 282us/sample - loss: 0.1226 - val_loss: 0.1082\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.10819 to 0.10104, saving model to SegLSTM.h5\n",
      "1707/1707 [==============================] - 0s 282us/sample - loss: 0.1156 - val_loss: 0.1010\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.10104 to 0.09643, saving model to SegLSTM.h5\n",
      "1707/1707 [==============================] - 0s 285us/sample - loss: 0.1103 - val_loss: 0.0964\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.09643 to 0.09437, saving model to SegLSTM.h5\n",
      "1707/1707 [==============================] - 0s 278us/sample - loss: 0.1075 - val_loss: 0.0944\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.09437 to 0.09404, saving model to SegLSTM.h5\n",
      "1707/1707 [==============================] - 0s 286us/sample - loss: 0.1066 - val_loss: 0.0940\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.09404\n",
      "1707/1707 [==============================] - 0s 273us/sample - loss: 0.1066 - val_loss: 0.0943\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.09404\n",
      "1707/1707 [==============================] - 0s 270us/sample - loss: 0.1063 - val_loss: 0.0941\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.09404 to 0.09326, saving model to SegLSTM.h5\n",
      "1707/1707 [==============================] - 1s 293us/sample - loss: 0.1047 - val_loss: 0.0933\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.09326 to 0.09190, saving model to SegLSTM.h5\n",
      "1707/1707 [==============================] - 0s 279us/sample - loss: 0.1020 - val_loss: 0.0919\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.09190 to 0.09052, saving model to SegLSTM.h5\n",
      "1707/1707 [==============================] - 0s 283us/sample - loss: 0.0984 - val_loss: 0.0905\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.09052 to 0.08957, saving model to SegLSTM.h5\n",
      "1707/1707 [==============================] - 1s 299us/sample - loss: 0.0945 - val_loss: 0.0896\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.08957 to 0.08932, saving model to SegLSTM.h5\n",
      "1707/1707 [==============================] - 0s 282us/sample - loss: 0.0910 - val_loss: 0.0893\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.08932\n",
      "1707/1707 [==============================] - 0s 275us/sample - loss: 0.0880 - val_loss: 0.0897\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.08932\n",
      "1707/1707 [==============================] - 1s 294us/sample - loss: 0.0858 - val_loss: 0.0903\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.08932\n",
      "1707/1707 [==============================] - 0s 287us/sample - loss: 0.0843 - val_loss: 0.0909\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.08932\n",
      "1707/1707 [==============================] - 0s 288us/sample - loss: 0.0833 - val_loss: 0.0911\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.08932\n",
      "1707/1707 [==============================] - 0s 292us/sample - loss: 0.0827 - val_loss: 0.0911\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.08932\n",
      "1707/1707 [==============================] - 0s 285us/sample - loss: 0.0820 - val_loss: 0.0910\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.08932\n",
      "1707/1707 [==============================] - 0s 290us/sample - loss: 0.0812 - val_loss: 0.0909\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.08932\n",
      "1707/1707 [==============================] - 0s 285us/sample - loss: 0.0802 - val_loss: 0.0907\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.08932\n",
      "1707/1707 [==============================] - 0s 280us/sample - loss: 0.0792 - val_loss: 0.0900\n",
      "Epoch 29/100\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.08932 to 0.08843, saving model to SegLSTM.h5\n",
      "1707/1707 [==============================] - 0s 286us/sample - loss: 0.0783 - val_loss: 0.0884\n",
      "Epoch 30/100\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.08843 to 0.08621, saving model to SegLSTM.h5\n",
      "1707/1707 [==============================] - 1s 296us/sample - loss: 0.0775 - val_loss: 0.0862\n",
      "Epoch 31/100\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.08621 to 0.08380, saving model to SegLSTM.h5\n",
      "1707/1707 [==============================] - 1s 307us/sample - loss: 0.0767 - val_loss: 0.0838\n",
      "Epoch 32/100\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.08380 to 0.08176, saving model to SegLSTM.h5\n",
      "1707/1707 [==============================] - 1s 302us/sample - loss: 0.0760 - val_loss: 0.0818\n",
      "Epoch 33/100\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.08176 to 0.08039, saving model to SegLSTM.h5\n",
      "1707/1707 [==============================] - 1s 294us/sample - loss: 0.0754 - val_loss: 0.0804\n",
      "Epoch 34/100\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.08039 to 0.07977, saving model to SegLSTM.h5\n",
      "1707/1707 [==============================] - 1s 301us/sample - loss: 0.0749 - val_loss: 0.0798\n",
      "Epoch 35/100\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.07977\n",
      "1707/1707 [==============================] - 0s 290us/sample - loss: 0.0743 - val_loss: 0.0798\n",
      "Epoch 36/100\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.07977\n",
      "1707/1707 [==============================] - 0s 286us/sample - loss: 0.0738 - val_loss: 0.0803\n",
      "Epoch 37/100\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.07977\n",
      "1707/1707 [==============================] - 0s 286us/sample - loss: 0.0733 - val_loss: 0.0809\n",
      "Epoch 38/100\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.07977\n",
      "1707/1707 [==============================] - 0s 287us/sample - loss: 0.0729 - val_loss: 0.0815\n",
      "Epoch 39/100\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.07977\n",
      "1707/1707 [==============================] - 0s 285us/sample - loss: 0.0724 - val_loss: 0.0820\n",
      "Epoch 40/100\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.07977\n",
      "1707/1707 [==============================] - 0s 287us/sample - loss: 0.0719 - val_loss: 0.0827\n",
      "Epoch 41/100\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.07977\n",
      "1707/1707 [==============================] - 0s 291us/sample - loss: 0.0713 - val_loss: 0.0835\n",
      "Epoch 42/100\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.07977\n",
      "1707/1707 [==============================] - 0s 284us/sample - loss: 0.0707 - val_loss: 0.0843\n",
      "Epoch 43/100\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.07977\n",
      "1707/1707 [==============================] - 0s 288us/sample - loss: 0.0703 - val_loss: 0.0843\n",
      "Epoch 44/100\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.07977\n",
      "1707/1707 [==============================] - 0s 283us/sample - loss: 0.0699 - val_loss: 0.0831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.07977\n",
      "1707/1707 [==============================] - 0s 279us/sample - loss: 0.0696 - val_loss: 0.0815\n",
      "Epoch 46/100\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.07977\n",
      "1707/1707 [==============================] - 0s 284us/sample - loss: 0.0692 - val_loss: 0.0806\n",
      "Epoch 47/100\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.07977\n",
      "1707/1707 [==============================] - 0s 284us/sample - loss: 0.0687 - val_loss: 0.0805\n",
      "Epoch 48/100\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.07977\n",
      "1707/1707 [==============================] - 0s 289us/sample - loss: 0.0683 - val_loss: 0.0801\n",
      "Epoch 49/100\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.07977 to 0.07905, saving model to SegLSTM.h5\n",
      "1707/1707 [==============================] - 0s 292us/sample - loss: 0.0680 - val_loss: 0.0790\n",
      "Epoch 50/100\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.07905 to 0.07897, saving model to SegLSTM.h5\n",
      "1707/1707 [==============================] - 0s 291us/sample - loss: 0.0679 - val_loss: 0.0790\n",
      "Epoch 51/100\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.07897\n",
      "1707/1707 [==============================] - 0s 278us/sample - loss: 0.0677 - val_loss: 0.0797\n",
      "Epoch 52/100\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.07897\n",
      "1707/1707 [==============================] - 0s 288us/sample - loss: 0.0675 - val_loss: 0.0797\n",
      "Epoch 53/100\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.07897\n",
      "1707/1707 [==============================] - 0s 289us/sample - loss: 0.0673 - val_loss: 0.0796\n",
      "Epoch 54/100\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.07897 to 0.07829, saving model to SegLSTM.h5\n",
      "1707/1707 [==============================] - 1s 305us/sample - loss: 0.0673 - val_loss: 0.0783\n",
      "Epoch 55/100\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.07829 to 0.07761, saving model to SegLSTM.h5\n",
      "1707/1707 [==============================] - 1s 294us/sample - loss: 0.0673 - val_loss: 0.0776\n",
      "Epoch 56/100\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.07761\n",
      "1707/1707 [==============================] - 0s 285us/sample - loss: 0.0672 - val_loss: 0.0782\n",
      "Epoch 57/100\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.07761 to 0.07303, saving model to SegLSTM.h5\n",
      "1707/1707 [==============================] - 1s 293us/sample - loss: 0.0670 - val_loss: 0.0730\n",
      "Epoch 58/100\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.07303\n",
      "1707/1707 [==============================] - 0s 288us/sample - loss: 0.0674 - val_loss: 0.0758\n",
      "Epoch 59/100\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.07303\n",
      "1707/1707 [==============================] - 0s 287us/sample - loss: 0.0668 - val_loss: 0.0816\n",
      "Epoch 60/100\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.07303\n",
      "1707/1707 [==============================] - 0s 288us/sample - loss: 0.0671 - val_loss: 0.0787\n",
      "Epoch 61/100\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.07303\n",
      "1707/1707 [==============================] - 0s 288us/sample - loss: 0.0665 - val_loss: 0.0740\n",
      "Epoch 62/100\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.07303\n",
      "1707/1707 [==============================] - 0s 286us/sample - loss: 0.0665 - val_loss: 0.0746\n",
      "Epoch 63/100\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.07303\n",
      "1707/1707 [==============================] - 0s 288us/sample - loss: 0.0663 - val_loss: 0.0791\n",
      "Epoch 64/100\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.07303\n",
      "1707/1707 [==============================] - 0s 281us/sample - loss: 0.0664 - val_loss: 0.0789\n",
      "Epoch 65/100\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.07303\n",
      "1707/1707 [==============================] - 0s 285us/sample - loss: 0.0664 - val_loss: 0.0753\n",
      "Epoch 66/100\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.07303\n",
      "1707/1707 [==============================] - 0s 292us/sample - loss: 0.0661 - val_loss: 0.0741\n",
      "Epoch 67/100\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.07303\n",
      "1707/1707 [==============================] - 0s 289us/sample - loss: 0.0662 - val_loss: 0.0768\n",
      "Epoch 68/100\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.07303\n",
      "1707/1707 [==============================] - 0s 285us/sample - loss: 0.0659 - val_loss: 0.0787\n",
      "Epoch 69/100\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.07303\n",
      "1707/1707 [==============================] - 0s 286us/sample - loss: 0.0661 - val_loss: 0.0760\n",
      "Epoch 70/100\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.07303\n",
      "1707/1707 [==============================] - 0s 283us/sample - loss: 0.0659 - val_loss: 0.0742\n",
      "Epoch 71/100\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.07303\n",
      "1707/1707 [==============================] - 0s 283us/sample - loss: 0.0659 - val_loss: 0.0756\n",
      "Epoch 72/100\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.07303\n",
      "1707/1707 [==============================] - 0s 286us/sample - loss: 0.0657 - val_loss: 0.0775\n",
      "Epoch 73/100\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.07303\n",
      "1707/1707 [==============================] - 0s 286us/sample - loss: 0.0657 - val_loss: 0.0764\n",
      "Epoch 74/100\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.07303\n",
      "1707/1707 [==============================] - 0s 290us/sample - loss: 0.0656 - val_loss: 0.0743\n",
      "Epoch 75/100\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.07303\n",
      "1707/1707 [==============================] - 0s 283us/sample - loss: 0.0655 - val_loss: 0.0750\n",
      "Epoch 76/100\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.07303\n",
      "1707/1707 [==============================] - 0s 289us/sample - loss: 0.0654 - val_loss: 0.0771\n",
      "Epoch 77/100\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.07303\n",
      "1707/1707 [==============================] - 0s 290us/sample - loss: 0.0654 - val_loss: 0.0764\n",
      "Epoch 78/100\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.07303\n",
      "1707/1707 [==============================] - 0s 286us/sample - loss: 0.0653 - val_loss: 0.0741\n",
      "Epoch 79/100\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.07303\n",
      "1707/1707 [==============================] - 0s 291us/sample - loss: 0.0653 - val_loss: 0.0739\n",
      "Epoch 80/100\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.07303\n",
      "1707/1707 [==============================] - 0s 272us/sample - loss: 0.0652 - val_loss: 0.0759\n",
      "Epoch 81/100\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.07303\n",
      "1707/1707 [==============================] - 0s 277us/sample - loss: 0.0651 - val_loss: 0.0761\n",
      "Epoch 82/100\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.07303\n",
      "1707/1707 [==============================] - 0s 274us/sample - loss: 0.0651 - val_loss: 0.0738\n",
      "Epoch 83/100\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.07303\n",
      "1707/1707 [==============================] - 0s 277us/sample - loss: 0.0651 - val_loss: 0.0741\n",
      "Epoch 84/100\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.07303\n",
      "1707/1707 [==============================] - 0s 272us/sample - loss: 0.0650 - val_loss: 0.0763\n",
      "Epoch 85/100\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.07303\n",
      "1707/1707 [==============================] - 0s 273us/sample - loss: 0.0649 - val_loss: 0.0763\n",
      "Epoch 86/100\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.07303\n",
      "1707/1707 [==============================] - 0s 278us/sample - loss: 0.0648 - val_loss: 0.0745\n",
      "Epoch 87/100\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.07303\n",
      "1707/1707 [==============================] - 0s 275us/sample - loss: 0.0647 - val_loss: 0.0749\n",
      "Epoch 88/100\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.07303\n",
      "1707/1707 [==============================] - 0s 273us/sample - loss: 0.0646 - val_loss: 0.0753\n",
      "Epoch 89/100\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.07303 to 0.07187, saving model to SegLSTM.h5\n",
      "1707/1707 [==============================] - 0s 284us/sample - loss: 0.0646 - val_loss: 0.0719\n",
      "Epoch 90/100\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.07187\n",
      "1707/1707 [==============================] - 0s 273us/sample - loss: 0.0652 - val_loss: 0.0741\n",
      "Epoch 91/100\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.07187\n",
      "1707/1707 [==============================] - 0s 272us/sample - loss: 0.0647 - val_loss: 0.0818\n",
      "Epoch 92/100\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.07187\n",
      "1707/1707 [==============================] - 0s 274us/sample - loss: 0.0654 - val_loss: 0.0787\n",
      "Epoch 93/100\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.07187\n",
      "1707/1707 [==============================] - 0s 275us/sample - loss: 0.0647 - val_loss: 0.0719\n",
      "Epoch 94/100\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.07187\n",
      "1707/1707 [==============================] - 0s 274us/sample - loss: 0.0650 - val_loss: 0.0725\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00095: val_loss did not improve from 0.07187\n",
      "1707/1707 [==============================] - 0s 274us/sample - loss: 0.0645 - val_loss: 0.0777\n",
      "Epoch 96/100\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.07187\n",
      "1707/1707 [==============================] - 0s 271us/sample - loss: 0.0647 - val_loss: 0.0767\n",
      "Epoch 97/100\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.07187 to 0.07068, saving model to SegLSTM.h5\n",
      "1707/1707 [==============================] - 0s 278us/sample - loss: 0.0644 - val_loss: 0.0707\n",
      "Epoch 98/100\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.07068\n",
      "1707/1707 [==============================] - 0s 270us/sample - loss: 0.0661 - val_loss: 0.0727\n",
      "Epoch 99/100\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.07068\n",
      "1707/1707 [==============================] - 0s 271us/sample - loss: 0.0652 - val_loss: 0.0823\n",
      "Epoch 100/100\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.07068\n",
      "1707/1707 [==============================] - 0s 275us/sample - loss: 0.0647 - val_loss: 0.0848\n",
      "Score on validation set\n",
      "446/1 [====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 123us/sample - loss: 0.0848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08481898903846741\n",
      "Score on test set\n",
      "659/1 [==========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 117us/sample - loss: 0.0931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0930740162730217\n"
     ]
    }
   ],
   "source": [
    "# design network\n",
    "seed(7)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(x_train.shape[1], x_train.shape[2]),return_sequences=True))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "print(model.summary())\n",
    "# fit network\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 1707\n",
    "model_checkpoint = ModelCheckpoint('SegLSTM.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False)\n",
    "csv_logger = CSVLogger('SegLSTM.log')\n",
    "model.fit(x_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(x_val, y_val), verbose=1, shuffle=False,callbacks=[csv_logger,model_checkpoint])\n",
    "print('Score on validation set')\n",
    "score = model.evaluate(x_val, y_val, batch_size=BATCH_SIZE)\n",
    "print(score)\n",
    "print('Score on test set')\n",
    "score = model.evaluate(x_test,y_test, batch_size=BATCH_SIZE)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv('SegLSTM.log',sep=',')\n",
    "print(np.shape(df))\n",
    "x = np.arange(1,EPOCHS+1,1)\n",
    "yt = df['loss'].values\n",
    "yv = df['val_loss'].values\n",
    "plt.plot(x,yt,label='Training Loss')\n",
    "plt.plot(x,yv,label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Square Error Loss')\n",
    "plt.savefig('TrainingLogLSTM.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing loss on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(659, 1024, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(659, 1024, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(659, 1024)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(659, 1024)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.09307401503663452"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "testPredict = model.predict(x_test)\n",
    "display(testPredict.shape)\n",
    "display(y_test.shape)\n",
    "testPredict=testPredict.reshape(testPredict.shape[0],testPredict.shape[1])\n",
    "y_test=y_test.reshape(y_test.shape[0],y_test.shape[1])\n",
    "display(testPredict.shape)\n",
    "display(y_test.shape)\n",
    "display(mean_squared_error(y_test,testPredict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
